{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RoFormerTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RoFormerTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n",
      "Some weights of the model checkpoint at /home/kit/clustering4server_simple/resource/roformer-sim-small-chinese were not used when initializing RoFormerModel: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing RoFormerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RoFormerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "11/29/2023 11:36:08 - WARNING - bert4vec.bert4vec - Fail to import faiss. If you want to use faiss, install faiss through PyPI. Now the program continues with brute force search.\n",
      "11/29/2023 11:36:08 - INFO - bert4vec.bert4vec - Encoding embeddings for sentences\n",
      "11/29/2023 11:36:08 - INFO - bert4vec.bert4vec - Building index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "早上第一次见面 \n",
      "User:你喜欢旅游吗？   \n",
      "agent:我很喜欢旅游。我认为旅行可以让我放松身心，并学习到很多不同的文化。\n",
      "User:你曾经去过哪些地方旅行？   \n",
      "agent:我曾经去过欧洲和亚洲的一些国家。我很喜欢意大利的威尼斯，那里真的很美。\n",
      "User:听起来很不错！你最喜欢的旅行经历是什么？ \n",
      "agent:我的一次旅行经历是在中国的西藏。那里的自然风光和人文景观令人难以忘怀。\n",
      "\n",
      "next_time:\n",
      "user:我突然忘记你曾经去过哪些地方旅行过了，你能再说一遍吗？\n",
      "我曾经去过欧洲和亚洲的一些国家。我很喜欢意大利的威尼斯，那里真的很美。\n",
      "和一个新的user碰面：\n",
      "user:说一个你印象最深刻的旅游经历\n",
      "我的一次旅行经历是在中国的西藏。那里的自然风光和人文景观令人难以忘怀。\n"
     ]
    }
   ],
   "source": [
    "from bert4vec import Bert4Vec\n",
    "\n",
    "model_name_or_path = '/home/kit/clustering4server_simple/resource/roformer-sim-small-chinese'\n",
    "model = Bert4Vec(model_name_or_path=model_name_or_path)\n",
    "# threshold为最低相似度阈值，top_k为查找的近邻个数\n",
    "first_meet =\"\"\"\n",
    "早上第一次见面 \n",
    "User:你喜欢旅游吗？   \n",
    "agent:我很喜欢旅游。我认为旅行可以让我放松身心，并学习到很多不同的文化。\n",
    "User:你曾经去过哪些地方旅行？   \n",
    "agent:我曾经去过欧洲和亚洲的一些国家。我很喜欢意大利的威尼斯，那里真的很美。\n",
    "User:听起来很不错！你最喜欢的旅行经历是什么？ \n",
    "agent:我的一次旅行经历是在中国的西藏。那里的自然风光和人文景观令人难以忘怀。\n",
    "\"\"\"\n",
    "#记录对话内容为记忆\n",
    "key_list = [\"你喜欢旅游吗？\",\"你曾经去过哪些地方旅行？\",\"听起来很不错！你最喜欢的旅行经历是什么？\"]\n",
    "value_list = [\"我很喜欢旅游。我认为旅行可以让我放松身心，并学习到很多不同的文化。\",\"我曾经去过欧洲和亚洲的一些国家。我很喜欢意大利的威尼斯，那里真的很美。\",\"我的一次旅行经历是在中国的西藏。那里的自然风光和人文景观令人难以忘怀。\"]\n",
    "#建立索引\n",
    "model.build_index(key_list, ann_search=True, gpu_index=False, n_search=32)\n",
    "next_meet_question = \"我突然忘记你曾经去过哪些地方旅行过了，你能再说一遍吗？\"\n",
    "new_meet_question = \"说一个你印象最深刻的旅游经历\"\n",
    "results = model.search(queries=[next_meet_question,new_meet_question], threshold=0.6, top_k=1)                      \n",
    "print(first_meet)\n",
    "print(\"next_time:\")\n",
    "print(\"user:我突然忘记你曾经去过哪些地方旅行过了，你能再说一遍吗？\")\n",
    "print(value_list[key_list.index(results[0][0][0])])\n",
    "print(\"和一个新的user碰面：\")\n",
    "print(\"user:说一个你印象最深刻的旅游经历\")\n",
    "print(value_list[key_list.index(results[1][0][0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RoFormerTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RoFormerTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n",
      "Some weights of the model checkpoint at /home/kit/clustering4server_simple/resource/roformer-sim-small-chinese were not used when initializing RoFormerModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing RoFormerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RoFormerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from bert4vec import Bert4Vec\n",
    "model_name_or_path = '/home/kit/clustering4server_simple/resource/roformer-sim-small-chinese'\n",
    "model = Bert4Vec(model_name_or_path=model_name_or_path) \n",
    "key_list = [\"你喜欢旅游吗？\",\"你曾经去过哪些地方旅行？\",\"听起来很不错！你最喜欢的旅行经历是什么？\"]\n",
    "value_list = [\"我很喜欢旅游。我认为旅行可以让我放松身心，并学习到很多不同的文化。\",\"我曾经去过欧洲和亚洲的一些国家。我很喜欢意大利的威尼斯，那里真的很美。\",\"我的一次旅行经历是在中国的西藏。那里的自然风光和人文景观令人难以忘怀。\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_sentences_bert4vec(model,sentences):\n",
    "    a_vecs = model.encode(sentences, convert_to_numpy=True, normalize_to_unit=False, batch_size=1)\n",
    "    embeddings = a_vecs / (a_vecs ** 2).sum(axis=1, keepdims=True) ** 0.5\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 384)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = get_embedding_sentences_bert4vec(model,key_list)\n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 384)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np  \n",
    "  \n",
    "def concatenate_lists(list1, list2):  \n",
    "    # 将两个列表转换为numpy数组  \n",
    "    arr1 = np.array(list1)  \n",
    "    arr2 = np.array(list2)  \n",
    "  \n",
    "    # 使用numpy的concatenate函数连接两个数组  \n",
    "    arr3 = np.concatenate((arr1, arr2))  \n",
    "  \n",
    "    return arr3\n",
    "concatenate_lists(vector,vector).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = nn.Embedding(28996, 768) # embed_weight: [28996,768]\n",
    "a = tokenizer.batch_encode_plus(aim_token, # words: [4,77]\n",
    "                                    padding='max_length',\n",
    "                                    truncation=True,\n",
    "                                    max_length=77).input_ids\n",
    "b = embed(torch.tensor(a)) # [4,77,768]\n",
    "e = b[0]@torch.linalg.pinv(embed.weight) # 计算第一句话的tokens和embed_weight伪逆的最小余弦投影，也不余弦了直接求积s\n",
    "voc = tokenizer.get_vocab()\n",
    "dictionary = dict(zip(voc.values(),voc.keys()))\n",
    "words = [torch.argmax(i).numpy() for i in e] # 查找余弦投影最小的点位置\n",
    "words = [int(i) for i in vocs]\n",
    "sentence = [dictionary[i] for i in vocs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "embed = nn.Embedding(28996, 768) # embed_weight: [28996,768]\n",
    "aim_token = \"a b c\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "a = tokenizer.batch_encode_plus(aim_token, # words: [4,77]\n",
    "                                    padding='max_length',\n",
    "                                    truncation=True,\n",
    "                                    max_length=77).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 100, 100, 1989, 100, 100, 102]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "vector = tokenizer(\"你好，再见\")['input_ids']\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'，'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc = tokenizer.get_vocab()\n",
    "dictionary = dict(zip(voc.values(),voc.keys()))\n",
    "dictionary[1989]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
